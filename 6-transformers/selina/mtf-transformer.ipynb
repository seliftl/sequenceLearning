{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.3 64-bit ('base': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.3"
    },
    "interpreter": {
      "hash": "7d368088e3eaf58bcc50b86b31290cec15c3ddd55a30cebc467950b24c1c0e04"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3VfTrkpNpCF",
        "outputId": "16ceefc1-ee9a-463b-fd50-99714a25f70a"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\feitl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (4.8.0)\nRequirement already satisfied: regex!=2019.12.17 in c:\\users\\feitl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from transformers) (2021.4.4)\nRequirement already satisfied: requests in c:\\users\\feitl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from transformers) (2.25.1)\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\feitl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from transformers) (0.10.3)\nRequirement already satisfied: tqdm>=4.27 in c:\\users\\feitl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from transformers) (4.59.0)\nRequirement already satisfied: numpy>=1.17 in c:\\users\\feitl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from transformers) (1.20.2)\nRequirement already satisfied: sacremoses in c:\\users\\feitl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from transformers) (0.0.45)\nRequirement already satisfied: packaging in c:\\users\\feitl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from transformers) (20.9)\nRequirement already satisfied: pyyaml in c:\\users\\feitl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from transformers) (5.4.1)\nRequirement already satisfied: importlib-metadata in c:\\users\\feitl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from transformers) (3.10.0)\nRequirement already satisfied: filelock in c:\\users\\feitl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\nRequirement already satisfied: huggingface-hub==0.0.12 in c:\\users\\feitl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from transformers) (0.0.12)\nRequirement already satisfied: typing-extensions in c:\\users\\feitl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from huggingface-hub==0.0.12->transformers) (3.10.0.0)\nRequirement already satisfied: pyparsing>=2.0.2 in c:\\users\\feitl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from packaging->transformers) (2.4.7)\nRequirement already satisfied: zipp>=0.5 in c:\\users\\feitl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from importlib-metadata->transformers) (3.4.1)\nRequirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\feitl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->transformers) (4.0.0)\nRequirement already satisfied: idna<3,>=2.5 in c:\\users\\feitl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\feitl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.4)\nRequirement already satisfied: certifi>=2017.4.17 in c:\\users\\feitl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.5.30)\nRequirement already satisfied: click in c:\\users\\feitl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (8.0.1)\nRequirement already satisfied: joblib in c:\\users\\feitl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.0.1)\nRequirement already satisfied: six in c:\\users\\feitl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.16.0)\nRequirement already satisfied: colorama in c:\\users\\feitl\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from click->sacremoses->transformers) (0.4.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4rtVi8YNz31",
        "outputId": "96854ee5-2590-4942-9ad4-71e103c7da7d"
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auKrL8zYN5eM"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import string\n",
        "import re\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import random\n",
        "\n",
        "# huggingface\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeAR4N9BhCTq"
      },
      "source": [
        "# data logistics: load theses title and abstract\n",
        "# limit_title_len=[4,10] restricts to titles in between 4 and 10 tokens\n",
        "def load_thesis_data(path='../theses.tsv', limit_title_len=None):\n",
        "    df = pd.read_csv(path, sep='\\t')\n",
        "    df = df[df['Sprache'] == 'DE']\n",
        "    df['length'] = df['Titel'].apply(lambda x: len(x.split()))\n",
        "    if limit_title_len != None:\n",
        "        df = df[df['length'].between(limit_title_len)] \n",
        "    return df\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCLtXG8ZhI-B"
      },
      "source": [
        "# set up the models; they will download on first time use but this will take some time (1.2 GB)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ml6team/mt5-small-german-finetune-mlsum\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"ml6team/mt5-small-german-finetune-mlsum\")\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HGD3-_uhNiv"
      },
      "source": [
        "\n",
        "# method for summary generation, using the global model and tokenizer\n",
        "def generate_summary(model, abstract, num_beams = 2, repetition_penalty = 2.5,\n",
        "                    length_penalty = 2.0, early_stopping = True, max_output_length = 150):\n",
        "    source_encoding=tokenizer(abstract, max_length=784, padding=\"max_length\", truncation=True, return_attention_mask=True, add_special_tokens=True, return_tensors=\"pt\")\n",
        "\n",
        "    generated_ids=model.generate(\n",
        "        input_ids=source_encoding[\"input_ids\"],\n",
        "        attention_mask=source_encoding[\"attention_mask\"],\n",
        "        num_beams=num_beams,\n",
        "        max_length=max_output_length,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        length_penalty=length_penalty,\n",
        "        early_stopping=early_stopping,\n",
        "        use_cache=True\n",
        "        )\n",
        "\n",
        "    # TODO ...map to string using tokenizer.decode and return\n",
        "    preds=[tokenizer.decode(gen_id, skip_special_tokens=True, clean_up_tokenization_spaces=True) \n",
        "         for gen_id in generated_ids]\n",
        "\n",
        "    return \"\".join(preds)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-8g59E6hOSg"
      },
      "source": [
        "# main program\n",
        "df = load_thesis_data()\n",
        "\n",
        "# Google Colab crashed on more than 5 thesis\n",
        "df = df[:5]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "id": "Mx1eNYxehR4L",
        "outputId": "636517aa-8dca-42c4-96e1-0db6289b7c8a"
      },
      "source": [
        "# now use the pre-trained model to generate some short summaries from the \n",
        "# abstracts, and compare them to the reference titles\n",
        "\n",
        "# adjust these values as desired\n",
        "num_beams = 2\n",
        "repetition_penalty = 1.0\n",
        "length_penalty = 2.0\n",
        "max_output_length = 120\n",
        "early_stopping = True\n",
        "\n",
        "# sample from dataset, using abstracts as input to generate short summary (~title)\n",
        "from IPython.display import HTML, display\n",
        "def displaysum(summarize, generated, reference):\n",
        "    display(HTML(f\"\"\"<table>\n",
        "    <tr><td>summarize:</td><td>{summarize}</td></tr>\n",
        "    <tr><td>generated:</td><td>{generated}</td></tr>\n",
        "    <tr><td>reference:</td><td>{reference}</td></tr>\n",
        "    </table>\n",
        "    \"\"\"))\n",
        "\n",
        "for i in [random.randint(0, len(df) - 1) for _ in range(10)]:\n",
        "    # load the values\n",
        "    summarize = df.iloc[i].Abstract\n",
        "    reference = df.iloc[i].Titel\n",
        "\n",
        "    generated = generate_summary(model, summarize, num_beams, repetition_penalty, length_penalty, early_stopping, max_output_length)\n",
        "\n",
        "    displaysum(summarize, generated, reference)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table>\n",
              "    <tr><td>summarize:</td><td>Fraud und Betrüge über digitale Dienste sind für Finanzinstitute ein ernstes Problem. Im Zuge des Wandels der Bankenlandschaft werden digitale Dienste in Zukunft weiter in den Vordergrund für Banken rücken. Banken möchten hier so schnell wie möglich betrügerische Transaktionen stoppen können, um Kundenauswirkungen oder finanziellen Schäden der Bank und des Kunden entegegenzuwirken. Eine maschinelle Analyse von Transaktionsdaten ist hierbei aufgrund der Menge an Daten unerlässlich. Diese Arbeit beschäftigt sich mit der Analyse von Logdaten aus dem Online Banking der ING-DiBa AG und wie eine Frauderkennung mithilfe der Big Data Streaming APIs Apache Flink und Apache Spark ermöglicht werden könnte. Elementar ist hierbei die Implementation von Session Windows durch die genannten Applikationen. Es wird zunächst ein Überblick über nötige Kenntnisse für Big-Data-Systeme geschaffen und Fraud im Kontext der ING-DiBa AG spezifiziert, bevor ein Beweis der Umsetzungsfähigkeit der beiden Frameworks betrachtet wird. Darüber hinaus wird ein Vorschlag zur Visualisierung der Daten in Echtzeit gegeben, und vorgestellt wie ein produktiver Aufbau eines Systems aussehen könnte. Die Evaluation wurde mit einem Benchmark und verschiedener Konfigurationen von Multi-Node Clustern durchgeführt, wobei Apache Flink schneller Abschnitt als Apache Spark.</td></tr>\n",
              "    <tr><td>generated:</td><td>Fraud und Betrüge über digitale Dienste sind für Finanzinstitute ein ernstes Problem.</td></tr>\n",
              "    <tr><td>reference:</td><td> Evaluierung von Apache Flink und Apache Spark im Kontext der Echtzeitanalyse von Weblogdaten auf Fraud bei der ING-DiBa AG. </td></tr>\n",
              "    </table>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-851b15855550>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mreference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTitel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_penalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_output_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mdisplaysum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummarize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-d610dcf77778>\u001b[0m in \u001b[0;36mgenerate_summary\u001b[0;34m(model, abstract, num_beams, repetition_penalty, length_penalty, early_stopping, max_output_length)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mlength_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlength_penalty\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         )\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1061\u001b[0m                 \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m                 \u001b[0msynced_gpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynced_gpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m             )\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1792\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1793\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1794\u001b[0;31m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1795\u001b[0m             )\n\u001b[1;32m   1796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1615\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1616\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1617\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1618\u001b[0m         )\n\u001b[1;32m   1619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1005\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m                     \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m                 )\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    670\u001b[0m                 \u001b[0mquery_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m             )\n\u001b[1;32m    674\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions)\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m             \u001b[0mquery_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m         )\n\u001b[1;32m    589\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    501\u001b[0m                 \u001b[0mposition_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposition_bias\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmask\u001b[0m  \u001b[0;31m# (batch_size, n_heads, seq_length, key_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mposition_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m         attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(\n\u001b[1;32m    505\u001b[0m             \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dN6sLdcPc6k6",
        "outputId": "9b222105-5247-448c-81d9-8eeb0d085219"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cpu'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jglh2AOfc9ik"
      },
      "source": [
        "class ThesisDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_input_len, max_output_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.source_len = max_input_len\n",
        "        self.summ_len = max_output_len\n",
        "        self.Titel = df.Titel\n",
        "\n",
        "        # T5 requires us to prepend the task\n",
        "        self.Abstract = 'summarize: ' + df.Abstract\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.Titel)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        abstract = str(self.Abstract[index])\n",
        "        title = str(self.Titel[index])\n",
        "\n",
        "        source_tok = self.tokenizer.batch_encode_plus([abstract], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n",
        "        label_tok = self.tokenizer.batch_encode_plus([title], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n",
        "\n",
        "        input_ids = source_tok['input_ids'].squeeze()\n",
        "        input_mask = source_tok['attention_mask'].squeeze()\n",
        "        label_ids = label_tok['input_ids'].squeeze()\n",
        "        label_mask = label_tok['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids.to(dtype=torch.long), \n",
        "            'input_mask': input_mask.to(dtype=torch.long), \n",
        "            'label_ids': label_ids.to(dtype=torch.long),\n",
        "            'label_mask': label_mask.to(dtype=torch.long)\n",
        "        }"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCRt-hSFdBWz"
      },
      "source": [
        "# for each point in the data loader, compute the forward pass, loss and\n",
        "# backward pass\n",
        "\n",
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "    model.train()\n",
        "\n",
        "    for i, data in enumerate(loader, 0):\n",
        "        y = data['label_ids'].to(device, dtype=torch.long)\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        lm_labels = y[:, 1:].clone().detach()\n",
        "\n",
        "        # set the padding symbols to -100 to be ignored by torch\n",
        "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "\n",
        "        inputs = data['input_ids'].to(device, dtype=torch.long)\n",
        "        mask = data['input_mask'].to(device, dtype=torch.long)\n",
        "\n",
        "        outputs = model(input_ids = inputs, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
        "\n",
        "        loss = outputs[0]\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print({\"Training Loss\": loss.item()})\n",
        "  \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    print(f'Epoch: {epoch}, Loss:  {loss.item()}')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajGCJGaIdEkv"
      },
      "source": [
        "# for validation, set the model to eval mode and compute all predictions\n",
        "def validate(epoch, tokenizer, model, device, loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(loader, 0):\n",
        "            y = data['label_ids'].to(device, dtype=torch.long)\n",
        "            ids = data['input_ids'].to(device, dtype=torch.long)\n",
        "            mask = data['input_mask'].to(device, dtype=torch.long)\n",
        "\n",
        "            generated_ids = model.generate(input_ids=ids,\n",
        "                attention_mask=mask,\n",
        "                num_beams=num_beams,\n",
        "                max_length=max_output_length,\n",
        "                repetition_penalty=repetition_penalty,\n",
        "                length_penalty=length_penalty,\n",
        "                early_stopping=early_stopping,\n",
        "                use_cache=True)\n",
        "            \n",
        "            preds=[tokenizer.decode(gen_id, skip_special_tokens=True, clean_up_tokenization_spaces=True) for gen_id in generated_ids]\n",
        "            target = [tokenizer.decode(gen_id, skip_special_tokens=True, clean_up_tokenization_spaces=True) for gen_id in y]\n",
        "            if i % 100 == 0:\n",
        "                print(f'Completed {i}')\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            actuals.extend(target)\n",
        "    \n",
        "    return predictions, actuals"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJIWJMO5dJ7_",
        "outputId": "e979cec0-49ac-4e2b-b34c-2dddceac7ddf"
      },
      "source": [
        "# defining some parameters that will be used later on in the training  \n",
        "batch_size_train = 32\n",
        "batch_size_vali = 4\n",
        "\n",
        "max_input_len = 512    # 512?\n",
        "max_output_len = 120\n",
        "\n",
        "# set random seeds and deterministic pytorch for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "if tokenizer is None:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"ml6team/mt5-small-german-finetune-mlsum\")\n",
        "\n",
        "if df is None:\n",
        "    df = load_thesis_data()\n",
        "    \n",
        "# split the dataframe into training and validation\n",
        "df_train = df.sample(frac=0.8, random_state=seed)\n",
        "df_vali = df.drop(df_train.index).reset_index(drop=True)\n",
        "df_train = df_train.reset_index(drop=True)\n",
        "print(\"df={df.shape}, train={df_train.shape}, vali={df_vali.shape}\")\n",
        "\n",
        "# Creating the Training and Validation dataset for further creation of Dataloader\n",
        "ds_train = ThesisDataset(df_train, tokenizer, max_input_len, max_output_len)\n",
        "ds_vali = ThesisDataset(df_vali, tokenizer, max_input_len, max_output_len)\n",
        "\n",
        "# create data loaders for training and validation\n",
        "from torch.utils.data import DataLoader\n",
        "dl_train = DataLoader(ds_train, shuffle=True, num_workers=0, batch_size=batch_size_train)\n",
        "dl_vali = DataLoader(ds_vali, shuffle=True, num_workers=0, batch_size=batch_size_vali)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df={df.shape}, train={df_train.shape}, vali={df_vali.shape}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBqe0wzZdQQj"
      },
      "source": [
        "# we'll start from the same ml6team/mt5-small-german-finetune-mlsum that we\n",
        "# used before in our baseline experiment; we will reload it below so that we\n",
        "# maintain the base model\n",
        "base = model\n",
        "\n",
        "# this time, we'll load it explicitly as a T5ForConditionalGeneration; the\n",
        "# tokenizer will be the same\n",
        "from transformers import T5ForConditionalGeneration\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"ml6team/mt5-small-german-finetune-mlsum\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-4)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3iTv60udVPH",
        "outputId": "124c53ca-9f65-4e77-e6d3-acf6eb77f19e"
      },
      "source": [
        "epochs_train = 3\n",
        "epochs_vali = [1, 2, 3]\n",
        "\n",
        "models = []\n",
        "\n",
        "for epoch in range(epochs_train):\n",
        "    train(epoch=epoch, tokenizer=tokenizer, model=model, device=device, loader=dl_train, optimizer=optimizer)\n",
        "\n",
        "    # save the model after each epoch; warning: model size is ~1.2G\n",
        "    #model.save_pretrained('res/mt5-small-fine-tune-'+epoch)\n",
        "\n",
        "    if epoch in epochs_vali:\n",
        "        predictions, actuals = validate(epoch, tokenizer, model, device, dl_vali)\n",
        "\n",
        "        # display some...\n",
        "        for i in [random.randint(0, len(predictions) - 1) for _ in range(10)]:\n",
        "            displaysum(None, generated, reference)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "7kWuyUkJgqMp",
        "outputId": "e061f355-8c29-458c-c455-e96913de9514"
      },
      "source": [
        "# save last iteration\n",
        "model.save_pretrained('res/mt5-small-fine-tune-theses')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-010191511ca9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# save last iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'res/mt5-small-fine-tune-theses'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsIul8nkgtWp"
      },
      "source": [
        "# load model and compare outputs\n",
        "base = AutoModelForSeq2SeqLM.from_pretrained(\"ml6team/mt5-small-german-finetune-mlsum\")\n",
        "fine = model  # or any other checkpoint from res/mt5-small-fine-tune-..."
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TUFHEVYAgym3",
        "outputId": "b7a262af-441e-476f-9fb6-7b60a5b02f2d"
      },
      "source": [
        "# pick some random theses (from df_vali!) and compare the two models\n",
        "thesis_picks = [random.randint(0, len(df_vali) - 1) for _ in range(10)]\n",
        "for num, i in enumerate(thesis_picks):\n",
        "    print()\n",
        "\n",
        "    # TODO generate a summary with each of the models\n",
        "    s1 = generate_summary(base, summarize, num_beams, repetition_penalty,\n",
        "                        \t    length_penalty, early_stopping, max_output_length)\n",
        "    s2 = generate_summary(fine, summarize, num_beams, repetition_penalty,\n",
        "                        \t    length_penalty, early_stopping, max_output_length)\n",
        "    \n",
        "    display(HTML(f\"\"\"<table>\n",
        "    <tr><td>summarize:</td><td>{df_vali.iloc[i].Abstract}</td></tr>\n",
        "    <tr><td>base:</td><td>{s1}</td></tr>\n",
        "    <tr><td>fine:</td><td>{s2}</td></tr>\n",
        "    <tr><td>reference:</td><td>{df_vali.iloc[i].Titel}</td></tr>\n",
        "    </table>\n",
        "    \"\"\"))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table>\n",
              "    <tr><td>summarize:</td><td>Ziel der Masterarbeit ist es, eine mobile Anwendung zur optischen Erfassung von Zählerinformationen zu konzeptionieren und Implementieren. Dafür sollen Zählerinformationen, wie z. B. Identifikationsnummer und Zählerstand, optisch erfasst und verarbeitet werden. Des Weiteren soll es möglich sein, der Erfassungssoftware IZAR@MOBILE ein Foto vom Einbauort des entsprechenden Zählers zuzuordnen. Nachdem alle relevanten Daten aufgenommen sind, werden sie an die zentrale Erfassungs- und Auswertungssoftware IZAR@NET verschlüsselt übertragen. Dabei ist IZAR@MOBILE eine Software, die eine einfache und wirtschaftliche Auslesung von Funkzählern über mobile Endgeräte ermöglicht. Für die Verwaltung der einzelnen Zähler wird die IZAR@NET verwendet. Zuletzt soll eine Marktanalyse für verschiedene Länder über die aktuelle Verteilung von \"intelligenten Messsystemen\" und \"konventionellen Zählern\" durchgeführt werden. Zusätzlich zu der Marktanalyse soll eine Analyse über die ökonomische Verwendung der oben genannten Features durchgeführt werden.</td></tr>\n",
              "    <tr><td>base:</td><td>Im Fokus dieser Arbeit steht der Einsatz von Cyber-Physischen Systemen in der Montage.</td></tr>\n",
              "    <tr><td>fine:</td><td>Im Fokus dieser Arbeit steht der Einsatz von Cyber-Physischen Systemen in der Montage.</td></tr>\n",
              "    <tr><td>reference:</td><td>Konzeption und Implementierung einer mobilen Anwendung zur optischen Erfassung von Zählerinformationen</td></tr>\n",
              "    </table>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table>\n",
              "    <tr><td>summarize:</td><td>Ziel der Masterarbeit ist es, eine mobile Anwendung zur optischen Erfassung von Zählerinformationen zu konzeptionieren und Implementieren. Dafür sollen Zählerinformationen, wie z. B. Identifikationsnummer und Zählerstand, optisch erfasst und verarbeitet werden. Des Weiteren soll es möglich sein, der Erfassungssoftware IZAR@MOBILE ein Foto vom Einbauort des entsprechenden Zählers zuzuordnen. Nachdem alle relevanten Daten aufgenommen sind, werden sie an die zentrale Erfassungs- und Auswertungssoftware IZAR@NET verschlüsselt übertragen. Dabei ist IZAR@MOBILE eine Software, die eine einfache und wirtschaftliche Auslesung von Funkzählern über mobile Endgeräte ermöglicht. Für die Verwaltung der einzelnen Zähler wird die IZAR@NET verwendet. Zuletzt soll eine Marktanalyse für verschiedene Länder über die aktuelle Verteilung von \"intelligenten Messsystemen\" und \"konventionellen Zählern\" durchgeführt werden. Zusätzlich zu der Marktanalyse soll eine Analyse über die ökonomische Verwendung der oben genannten Features durchgeführt werden.</td></tr>\n",
              "    <tr><td>base:</td><td>Im Fokus dieser Arbeit steht der Einsatz von Cyber-Physischen Systemen in der Montage.</td></tr>\n",
              "    <tr><td>fine:</td><td>Im Fokus dieser Arbeit steht der Einsatz von Cyber-Physischen Systemen in der Montage.</td></tr>\n",
              "    <tr><td>reference:</td><td>Konzeption und Implementierung einer mobilen Anwendung zur optischen Erfassung von Zählerinformationen</td></tr>\n",
              "    </table>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table>\n",
              "    <tr><td>summarize:</td><td>Ziel der Masterarbeit ist es, eine mobile Anwendung zur optischen Erfassung von Zählerinformationen zu konzeptionieren und Implementieren. Dafür sollen Zählerinformationen, wie z. B. Identifikationsnummer und Zählerstand, optisch erfasst und verarbeitet werden. Des Weiteren soll es möglich sein, der Erfassungssoftware IZAR@MOBILE ein Foto vom Einbauort des entsprechenden Zählers zuzuordnen. Nachdem alle relevanten Daten aufgenommen sind, werden sie an die zentrale Erfassungs- und Auswertungssoftware IZAR@NET verschlüsselt übertragen. Dabei ist IZAR@MOBILE eine Software, die eine einfache und wirtschaftliche Auslesung von Funkzählern über mobile Endgeräte ermöglicht. Für die Verwaltung der einzelnen Zähler wird die IZAR@NET verwendet. Zuletzt soll eine Marktanalyse für verschiedene Länder über die aktuelle Verteilung von \"intelligenten Messsystemen\" und \"konventionellen Zählern\" durchgeführt werden. Zusätzlich zu der Marktanalyse soll eine Analyse über die ökonomische Verwendung der oben genannten Features durchgeführt werden.</td></tr>\n",
              "    <tr><td>base:</td><td>Im Fokus dieser Arbeit steht der Einsatz von Cyber-Physischen Systemen in der Montage.</td></tr>\n",
              "    <tr><td>fine:</td><td>Im Fokus dieser Arbeit steht der Einsatz von Cyber-Physischen Systemen in der Montage.</td></tr>\n",
              "    <tr><td>reference:</td><td>Konzeption und Implementierung einer mobilen Anwendung zur optischen Erfassung von Zählerinformationen</td></tr>\n",
              "    </table>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table>\n",
              "    <tr><td>summarize:</td><td>Ziel der Masterarbeit ist es, eine mobile Anwendung zur optischen Erfassung von Zählerinformationen zu konzeptionieren und Implementieren. Dafür sollen Zählerinformationen, wie z. B. Identifikationsnummer und Zählerstand, optisch erfasst und verarbeitet werden. Des Weiteren soll es möglich sein, der Erfassungssoftware IZAR@MOBILE ein Foto vom Einbauort des entsprechenden Zählers zuzuordnen. Nachdem alle relevanten Daten aufgenommen sind, werden sie an die zentrale Erfassungs- und Auswertungssoftware IZAR@NET verschlüsselt übertragen. Dabei ist IZAR@MOBILE eine Software, die eine einfache und wirtschaftliche Auslesung von Funkzählern über mobile Endgeräte ermöglicht. Für die Verwaltung der einzelnen Zähler wird die IZAR@NET verwendet. Zuletzt soll eine Marktanalyse für verschiedene Länder über die aktuelle Verteilung von \"intelligenten Messsystemen\" und \"konventionellen Zählern\" durchgeführt werden. Zusätzlich zu der Marktanalyse soll eine Analyse über die ökonomische Verwendung der oben genannten Features durchgeführt werden.</td></tr>\n",
              "    <tr><td>base:</td><td>Im Fokus dieser Arbeit steht der Einsatz von Cyber-Physischen Systemen in der Montage.</td></tr>\n",
              "    <tr><td>fine:</td><td>Im Fokus dieser Arbeit steht der Einsatz von Cyber-Physischen Systemen in der Montage.</td></tr>\n",
              "    <tr><td>reference:</td><td>Konzeption und Implementierung einer mobilen Anwendung zur optischen Erfassung von Zählerinformationen</td></tr>\n",
              "    </table>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table>\n",
              "    <tr><td>summarize:</td><td>Ziel der Masterarbeit ist es, eine mobile Anwendung zur optischen Erfassung von Zählerinformationen zu konzeptionieren und Implementieren. Dafür sollen Zählerinformationen, wie z. B. Identifikationsnummer und Zählerstand, optisch erfasst und verarbeitet werden. Des Weiteren soll es möglich sein, der Erfassungssoftware IZAR@MOBILE ein Foto vom Einbauort des entsprechenden Zählers zuzuordnen. Nachdem alle relevanten Daten aufgenommen sind, werden sie an die zentrale Erfassungs- und Auswertungssoftware IZAR@NET verschlüsselt übertragen. Dabei ist IZAR@MOBILE eine Software, die eine einfache und wirtschaftliche Auslesung von Funkzählern über mobile Endgeräte ermöglicht. Für die Verwaltung der einzelnen Zähler wird die IZAR@NET verwendet. Zuletzt soll eine Marktanalyse für verschiedene Länder über die aktuelle Verteilung von \"intelligenten Messsystemen\" und \"konventionellen Zählern\" durchgeführt werden. Zusätzlich zu der Marktanalyse soll eine Analyse über die ökonomische Verwendung der oben genannten Features durchgeführt werden.</td></tr>\n",
              "    <tr><td>base:</td><td>Im Fokus dieser Arbeit steht der Einsatz von Cyber-Physischen Systemen in der Montage.</td></tr>\n",
              "    <tr><td>fine:</td><td>Im Fokus dieser Arbeit steht der Einsatz von Cyber-Physischen Systemen in der Montage.</td></tr>\n",
              "    <tr><td>reference:</td><td>Konzeption und Implementierung einer mobilen Anwendung zur optischen Erfassung von Zählerinformationen</td></tr>\n",
              "    </table>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table>\n",
              "    <tr><td>summarize:</td><td>Ziel der Masterarbeit ist es, eine mobile Anwendung zur optischen Erfassung von Zählerinformationen zu konzeptionieren und Implementieren. Dafür sollen Zählerinformationen, wie z. B. Identifikationsnummer und Zählerstand, optisch erfasst und verarbeitet werden. Des Weiteren soll es möglich sein, der Erfassungssoftware IZAR@MOBILE ein Foto vom Einbauort des entsprechenden Zählers zuzuordnen. Nachdem alle relevanten Daten aufgenommen sind, werden sie an die zentrale Erfassungs- und Auswertungssoftware IZAR@NET verschlüsselt übertragen. Dabei ist IZAR@MOBILE eine Software, die eine einfache und wirtschaftliche Auslesung von Funkzählern über mobile Endgeräte ermöglicht. Für die Verwaltung der einzelnen Zähler wird die IZAR@NET verwendet. Zuletzt soll eine Marktanalyse für verschiedene Länder über die aktuelle Verteilung von \"intelligenten Messsystemen\" und \"konventionellen Zählern\" durchgeführt werden. Zusätzlich zu der Marktanalyse soll eine Analyse über die ökonomische Verwendung der oben genannten Features durchgeführt werden.</td></tr>\n",
              "    <tr><td>base:</td><td>Im Fokus dieser Arbeit steht der Einsatz von Cyber-Physischen Systemen in der Montage.</td></tr>\n",
              "    <tr><td>fine:</td><td>Im Fokus dieser Arbeit steht der Einsatz von Cyber-Physischen Systemen in der Montage.</td></tr>\n",
              "    <tr><td>reference:</td><td>Konzeption und Implementierung einer mobilen Anwendung zur optischen Erfassung von Zählerinformationen</td></tr>\n",
              "    </table>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table>\n",
              "    <tr><td>summarize:</td><td>Ziel der Masterarbeit ist es, eine mobile Anwendung zur optischen Erfassung von Zählerinformationen zu konzeptionieren und Implementieren. Dafür sollen Zählerinformationen, wie z. B. Identifikationsnummer und Zählerstand, optisch erfasst und verarbeitet werden. Des Weiteren soll es möglich sein, der Erfassungssoftware IZAR@MOBILE ein Foto vom Einbauort des entsprechenden Zählers zuzuordnen. Nachdem alle relevanten Daten aufgenommen sind, werden sie an die zentrale Erfassungs- und Auswertungssoftware IZAR@NET verschlüsselt übertragen. Dabei ist IZAR@MOBILE eine Software, die eine einfache und wirtschaftliche Auslesung von Funkzählern über mobile Endgeräte ermöglicht. Für die Verwaltung der einzelnen Zähler wird die IZAR@NET verwendet. Zuletzt soll eine Marktanalyse für verschiedene Länder über die aktuelle Verteilung von \"intelligenten Messsystemen\" und \"konventionellen Zählern\" durchgeführt werden. Zusätzlich zu der Marktanalyse soll eine Analyse über die ökonomische Verwendung der oben genannten Features durchgeführt werden.</td></tr>\n",
              "    <tr><td>base:</td><td>Im Fokus dieser Arbeit steht der Einsatz von Cyber-Physischen Systemen in der Montage.</td></tr>\n",
              "    <tr><td>fine:</td><td>Im Fokus dieser Arbeit steht der Einsatz von Cyber-Physischen Systemen in der Montage.</td></tr>\n",
              "    <tr><td>reference:</td><td>Konzeption und Implementierung einer mobilen Anwendung zur optischen Erfassung von Zählerinformationen</td></tr>\n",
              "    </table>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table>\n",
              "    <tr><td>summarize:</td><td>Ziel der Masterarbeit ist es, eine mobile Anwendung zur optischen Erfassung von Zählerinformationen zu konzeptionieren und Implementieren. Dafür sollen Zählerinformationen, wie z. B. Identifikationsnummer und Zählerstand, optisch erfasst und verarbeitet werden. Des Weiteren soll es möglich sein, der Erfassungssoftware IZAR@MOBILE ein Foto vom Einbauort des entsprechenden Zählers zuzuordnen. Nachdem alle relevanten Daten aufgenommen sind, werden sie an die zentrale Erfassungs- und Auswertungssoftware IZAR@NET verschlüsselt übertragen. Dabei ist IZAR@MOBILE eine Software, die eine einfache und wirtschaftliche Auslesung von Funkzählern über mobile Endgeräte ermöglicht. Für die Verwaltung der einzelnen Zähler wird die IZAR@NET verwendet. Zuletzt soll eine Marktanalyse für verschiedene Länder über die aktuelle Verteilung von \"intelligenten Messsystemen\" und \"konventionellen Zählern\" durchgeführt werden. Zusätzlich zu der Marktanalyse soll eine Analyse über die ökonomische Verwendung der oben genannten Features durchgeführt werden.</td></tr>\n",
              "    <tr><td>base:</td><td>Im Fokus dieser Arbeit steht der Einsatz von Cyber-Physischen Systemen in der Montage.</td></tr>\n",
              "    <tr><td>fine:</td><td>Im Fokus dieser Arbeit steht der Einsatz von Cyber-Physischen Systemen in der Montage.</td></tr>\n",
              "    <tr><td>reference:</td><td>Konzeption und Implementierung einer mobilen Anwendung zur optischen Erfassung von Zählerinformationen</td></tr>\n",
              "    </table>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table>\n",
              "    <tr><td>summarize:</td><td>Ziel der Masterarbeit ist es, eine mobile Anwendung zur optischen Erfassung von Zählerinformationen zu konzeptionieren und Implementieren. Dafür sollen Zählerinformationen, wie z. B. Identifikationsnummer und Zählerstand, optisch erfasst und verarbeitet werden. Des Weiteren soll es möglich sein, der Erfassungssoftware IZAR@MOBILE ein Foto vom Einbauort des entsprechenden Zählers zuzuordnen. Nachdem alle relevanten Daten aufgenommen sind, werden sie an die zentrale Erfassungs- und Auswertungssoftware IZAR@NET verschlüsselt übertragen. Dabei ist IZAR@MOBILE eine Software, die eine einfache und wirtschaftliche Auslesung von Funkzählern über mobile Endgeräte ermöglicht. Für die Verwaltung der einzelnen Zähler wird die IZAR@NET verwendet. Zuletzt soll eine Marktanalyse für verschiedene Länder über die aktuelle Verteilung von \"intelligenten Messsystemen\" und \"konventionellen Zählern\" durchgeführt werden. Zusätzlich zu der Marktanalyse soll eine Analyse über die ökonomische Verwendung der oben genannten Features durchgeführt werden.</td></tr>\n",
              "    <tr><td>base:</td><td>Im Fokus dieser Arbeit steht der Einsatz von Cyber-Physischen Systemen in der Montage.</td></tr>\n",
              "    <tr><td>fine:</td><td>Im Fokus dieser Arbeit steht der Einsatz von Cyber-Physischen Systemen in der Montage.</td></tr>\n",
              "    <tr><td>reference:</td><td>Konzeption und Implementierung einer mobilen Anwendung zur optischen Erfassung von Zählerinformationen</td></tr>\n",
              "    </table>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table>\n",
              "    <tr><td>summarize:</td><td>Ziel der Masterarbeit ist es, eine mobile Anwendung zur optischen Erfassung von Zählerinformationen zu konzeptionieren und Implementieren. Dafür sollen Zählerinformationen, wie z. B. Identifikationsnummer und Zählerstand, optisch erfasst und verarbeitet werden. Des Weiteren soll es möglich sein, der Erfassungssoftware IZAR@MOBILE ein Foto vom Einbauort des entsprechenden Zählers zuzuordnen. Nachdem alle relevanten Daten aufgenommen sind, werden sie an die zentrale Erfassungs- und Auswertungssoftware IZAR@NET verschlüsselt übertragen. Dabei ist IZAR@MOBILE eine Software, die eine einfache und wirtschaftliche Auslesung von Funkzählern über mobile Endgeräte ermöglicht. Für die Verwaltung der einzelnen Zähler wird die IZAR@NET verwendet. Zuletzt soll eine Marktanalyse für verschiedene Länder über die aktuelle Verteilung von \"intelligenten Messsystemen\" und \"konventionellen Zählern\" durchgeführt werden. Zusätzlich zu der Marktanalyse soll eine Analyse über die ökonomische Verwendung der oben genannten Features durchgeführt werden.</td></tr>\n",
              "    <tr><td>base:</td><td>Im Fokus dieser Arbeit steht der Einsatz von Cyber-Physischen Systemen in der Montage.</td></tr>\n",
              "    <tr><td>fine:</td><td>Im Fokus dieser Arbeit steht der Einsatz von Cyber-Physischen Systemen in der Montage.</td></tr>\n",
              "    <tr><td>reference:</td><td>Konzeption und Implementierung einer mobilen Anwendung zur optischen Erfassung von Zählerinformationen</td></tr>\n",
              "    </table>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}